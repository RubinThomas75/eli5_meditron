{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "collapsed_sections": [
        "EmEF_p1G3d5I"
      ],
      "authorship_tag": "ABX9TyMhpFc8mqeZWpeqcwlFALIa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RubinThomas75/eli5_meditron/blob/main/eli5_meditron_fine_tune.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Meditron Model Fine Tuning\n",
        "\n",
        "Downloaded weights from https://huggingface.co/epfl-llm/meditron-70b"
      ],
      "metadata": {
        "id": "zO7cAcpA1z0T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install Dependencies"
      ],
      "metadata": {
        "id": "M0JXYQgN1prf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YcPeOIaOHqaU",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install torch transformers accelerate datasets peft bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "token_file_path = \"/content/drive/MyDrive/hf_read_token.txt\"\n",
        "\n",
        "with open(token_file_path, \"r\", encoding=\"utf-8-sig\") as f:\n",
        "    token = f.read().strip()\n",
        "\n",
        "os.environ[\"HF_TOKEN\"] = token"
      ],
      "metadata": {
        "id": "iasc3uq7NUQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set up Lora"
      ],
      "metadata": {
        "id": "M4pmwuF03DDy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "cache_dir = \"/content/drive/MyDrive/epfLLM_meditron7b\"\n",
        "train_file = \"/content/drive/MyDrive/eli5_medical_train.jsonl\"\n",
        "val_file = \"/content/drive/MyDrive/eli5_medical_val.jsonl\"\n",
        "\n",
        "dataset = load_dataset(\"json\", data_files={\"train\": train_file, \"validation\": val_file})\n",
        "\n",
        "# Load Meditron 7B model and tokenizer\n",
        "model_name = \"epfl-llm/meditron-7b\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_name, cache_dir=cache_dir, use_auth_token=os.environ[\"HF_TOKEN\"]\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name, cache_dir=cache_dir, use_auth_token=os.environ[\"HF_TOKEN\"],\n",
        "    load_in_8bit=True, device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# Apply LoRA\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.1,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"]\n",
        ")\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "def tokenize_function(example):\n",
        "    tokens = tokenizer(\n",
        "        example[\"text\"], truncation=True, padding=\"max_length\", max_length=512\n",
        "    )\n",
        "    tokens[\"labels\"] = tokens[\"input_ids\"].copy()\n",
        "    return tokens\n",
        "\n",
        "\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "x4i3fmBeY9Bk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run Trainer"
      ],
      "metadata": {
        "id": "mtTQ8FF53HC7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/content/drive/MyDrive/meditron-lora-checkpoints\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    num_train_epochs=3,\n",
        "    logging_dir=\"/content/drive/MyDrive/logs\",\n",
        "    save_total_limit=2,\n",
        "    learning_rate=2e-5,\n",
        "    fp16=True,\n",
        "    push_to_hub=False\n",
        ")\n",
        "\n",
        "# Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    tokenizer=tokenizer\n",
        ")\n"
      ],
      "metadata": {
        "id": "VqkCdZJ52WjC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()\n",
        "\n",
        "model.save_pretrained(\"/content/drive/MyDrive/meditron-lora\")\n",
        "print(\"Training completed.\")\n",
        "\n",
        "tokenizer.save_pretrained(\"/content/drive/MyDrive/meditron-lora\")\n",
        "print(\"Model saved in Google Drive.\")"
      ],
      "metadata": {
        "id": "QxSUY--72bAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above step will save the adaptors. When loading, we need to merge the adaptor with model."
      ],
      "metadata": {
        "id": "y_aKvc1-3TWS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import PeftModel\n",
        "\n",
        "base_model_name = \"epfl-llm/meditron-7b\"\n",
        "adapter_path = \"/content/drive/MyDrive/meditron-lora\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(base_model_name, load_in_8bit=True, device_map=\"auto\")\n",
        "\n",
        "# Load LoRA adapter and merge\n",
        "model = PeftModel.from_pretrained(model, adapter_path)\n",
        "model = model.merge_and_unload()\n"
      ],
      "metadata": {
        "id": "rDp8zXyyiB8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run inference"
      ],
      "metadata": {
        "id": "RfWZRw8b3bTV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Stop token?\n",
        "model.config.pad_token_id = model.config.eos_token_id\n",
        "\n",
        "system_message = \"[SYSTEM]: You are a helpful medical assistant who explains complex topics in simple terms.\"\n",
        "user_question = \"What is a headache?\"\n",
        "input_text = f\"{system_message}\\n[USER]: {user_question}\\n[ASSISTANT]:\"\n",
        "\n",
        "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
        "\n",
        "# Generate response\n",
        "with torch.no_grad():\n",
        "    output_ids = model.generate(\n",
        "        input_ids,\n",
        "        max_length=300,\n",
        "        num_beams=3,\n",
        "        early_stopping=True,\n",
        "        no_repeat_ngram_size=3,\n",
        "        top_p=0.9,\n",
        "        top_k=50,\n",
        "        eos_token_id=model.config.eos_token_id\n",
        "    )\n",
        "\n",
        "decoded_output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"\\n=== Model Response ===\")\n",
        "print(decoded_output.replace(input_text, \"\").strip())  # Remove prompt from output"
      ],
      "metadata": {
        "id": "0oijI7RGmhHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Legacy, plaground and base model inference\n"
      ],
      "metadata": {
        "id": "EmEF_p1G3d5I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load EPFL LLM\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "model_name = \"epfl-llm/meditron-7b\"\n",
        "cache_dir = \"/content/drive/MyDrive/epfLLM_meditron7b\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_name,\n",
        "    cache_dir=cache_dir,\n",
        "    use_auth_token=os.environ[\"HF_TOKEN\"]\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    cache_dir=cache_dir,\n",
        "    use_auth_token=os.environ[\"HF_TOKEN\"]\n",
        ")\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "print(f\"Model loaded on device: {device}\")"
      ],
      "metadata": {
        "id": "o1vzHCLhJK4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define stop tokens (e.g., ### or newline-based termination)\n",
        "stop_token = \"###\"\n",
        "stop_token_id = tokenizer.encode(stop_token, add_special_tokens=False)[0]\n",
        "\n",
        "# Quick Test Inference\n",
        "input_text = \"A child asks: 'What is a headache?' Answer in a way a 5-year-old would understand. Example: 'A headache is when your head feels tight or ouchy. Sometimes it happens when you're too tired or didn't drink enough water.' Now, your answer:\"\n",
        "\n",
        "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
        "\n",
        "model.config.pad_token_id = model.config.eos_token_id\n",
        "\n",
        "# Generate attention mask\n",
        "attention_mask = torch.ones(input_ids.shape, device=device)\n",
        "\n",
        "# Generate the response with stop token enforcement\n",
        "with torch.no_grad():\n",
        "    output_ids = model.generate(\n",
        "        input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        max_length=200,         # Adjust for longer/shorter responses\n",
        "        num_beams=5,            # Increase for more exhaustive search\n",
        "        early_stopping=True,\n",
        "        no_repeat_ngram_size=2,\n",
        "        top_p=0.9,              # Use nucleus sampling\n",
        "        top_k=50,               # Use top-k sampling\n",
        "        eos_token_id=stop_token_id  # Force model to stop at the stop token\n",
        "    )\n",
        "\n",
        "# Decode the output\n",
        "decoded_output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "# Truncate response at the stop token if it appears\n",
        "if stop_token in decoded_output:\n",
        "    decoded_output = decoded_output.split(stop_token)[0]\n",
        "\n",
        "print(\"\\n=== Model Response ===\")\n",
        "print(decoded_output)\n"
      ],
      "metadata": {
        "id": "b1GxZpiKJqsu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}